{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part 1.  Building a Text Bayesian Classifier\n",
    "I wanted a way to automatically classify Wikipedia articles into certain categories.  For this, we'll be using Python 3.4, along with Python's Natural Language Toolkit (NLTK) and Wikipedia modules, both installable through pip (or pip3, as the case may be). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Defs\n",
    "Here we list the data that will define our \"TRAINING\" data set, the data we feed the machine to improve it's learning and help it make better decisions when presented with test cases later on.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAINERS = [{'person':['Bill_Clinton','George_W_Bush','Morgan_Freeman','Susan_Sarandon','Nicole_Kidman','Albert_Einstein',\n",
    "                       'Stephen_Hawking','Rand_Paul','Charlie_Sheen','Jennifer_Lawrence','Jimi_Hendrix','Simone_Simons',\n",
    "                       'Jesus_Christ','Immanuel_Kant','Eric_Idle','Robin_Williams','Oprah_Winfrey','Whoopi_Goldberg',\n",
    "                       'Kendra_Wilkinson','Kathie_Lee_Gifford','Rachael_Ray','Milton_Friedman','Rush_Limbaugh',\n",
    "                       'Alan_Colmes',\n",
    "                       'Sean_Hannity','Charles_Krauthammer','Ted_Nugent','Angus_Young','Axl_Rose','Dave_Mustaine',\n",
    "                       'Mick_Jagger','John_Lennon','Paul_McCartney','Ringo_Starr','George_Harrison','Keith_Richards',\n",
    "                       'Cindy_Crawford',                     \n",
    "                      ]},\n",
    "            {'city':['Anchorage','Auckland','Bali','Beijing','Boise','Boston','Calgary','Chicago','Dallas','Hiroshima',\n",
    "                     'Kabul','Kansas_City','London','Minneapolis','Mumbai',\n",
    "                     'New_Brunswick','New_York_City','Okinawa','Paris','Seattle','Soweto','Sydney','Tokyo',\n",
    "                    'Winnipeg','Spokane','Bozeman','Cincinnati','Charlotte','Atlanta','Orlando','Tampa',\n",
    "                    'Prague','Warsaw','Minsk','Vilnius','Tbilisi'\n",
    "                    ]},\n",
    "            {'movie':['Pretty_Woman','Star_Wars','Timecop','Billy_Bathgate','Die_Hard','12_Monkeys','28_Days',\n",
    "                      'Bull_Durham','Passion_of_The_Christ','Top_Gun','Brokeback_Mountain','Inception','Toy_Story',\n",
    "                      'Django_Unchained','The_Empire_Strikes_Back','The_Fifth_Element','Dawn_Of_The_Planet_Of_The_Apes',\n",
    "                     'Rio_2','How_To_Train_Your_Dragon_2','Big_Eyes','Still_Alice','The_Grand_Budapest_Hotel',\n",
    "                     'The_Lego_Movie','Jamesy_Boy','The_Nut_Job','The_Legend_Of_Hercules','Kidnapped_For_Christ',\n",
    "                     ]},\n",
    "            {'animal':['Orca','Golden_Retriever','Blue_Whale','Cockroach','Mouse','Goldfish','Chinook_Salmon','Dog',\n",
    "                       'Right_Whale','Snowshoe_Hare','Whitetail_Deer','Ruffed_Grouse','Canadian_Goose',\n",
    "                       'American_Black_Bear','Grizzly_Bear','Raccoon','Badger','Wolverine','Skunk','Weasel',\n",
    "                       'African_Elephant',\n",
    "                      'Asian_Elephant','Polar_Bear','Giraffe','Black_Mamba','Saltwater_Crocodile','Seasnake',\n",
    "                       'American_Alligator','Komodo_Dragon','Seahorse','Ferret','Snowshoe_Hare','Gerbil','Hamster',\n",
    "                       'Muskrat',\n",
    "                      'Crow','Bald_Eagle','Bluebird','Trilobite','Oyster','Jellyfish','Walleye','Great_Blue_Heron','Swan',\n",
    "                      ]},\n",
    "            {'study':['Astronomy','Physics','Mathematics','Kinesiology','Sociology','Political_theory','Psychology',\n",
    "                     'Chemistry','Radiology','Cardiology','Biochemistry','Cosmology','Feminism','Ethics','Biology',\n",
    "                     'Industrial_Engineering','Cosmetology','Graphic_Design','Interior_Design','Pharmacy','Nutrition',\n",
    "                      'Art',\n",
    "                     'Poetry','Literature','Humanities','Computer_Programming','Cooking','Architecture','Marine_Biology',\n",
    "                     ]},\n",
    "            {'holiday':['Christmas','Thanksgiving','Valentines_Day','Boxing_Day','Memorial_Day','Veterans_Day',\n",
    "                       'Columbus_Day','Flag_Day','Labor_Day','Armistice_Day','Victoria_Day','Rememberance_Day','Easter',\n",
    "                       'Discovery_Day','Orange_walk','Easter_Monday','Samhain','Halloween'\n",
    "                       ]}\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Test data definition\n",
    "This data comprises the testdata we want to feed later to the machine to test how well it has learned and see if it can classify articles correctly now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now, build a testset of data and possible categories to test the classifier against...\n",
    "CATEGORIES = [ 'person','city','animal','movie','study','holiday']\n",
    "TESTDATA = ['Abraham_Lincoln','Danny_Devito','Rheumatology','Oarfish','Lobster','United_Passions','Air_Heads',\n",
    "           'Benjamin_Franklin','Grace_Kelly','Blue_Whale','Grey_Seal','Glenn_Howerton','Charlie_Day','The_Last_Airbender',\n",
    "           'Susan_Sarandon','Joanna_Kerns','Snowy_Owl','Great_White_Shark','Mako_Shark','Mahi_Mahi']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Writing the code\n",
    "We use Python's NLTK (Natural Language Toolkit) and wikipedia modules for this exercise, along with the use of 'random' later on in the cross_validation functions. These modules are all we need to get our machine learning classifier working.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "import wikipedia\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Trainer_length function\n",
    "This really only shows the length of each training list and is there for our own convenience when training the data set.  Overfitting can become a real issue and knowing the length of each list can be useful at these times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainer_length(trainer):\n",
    "    for t in trainer:\n",
    "        for key, vals in t.items():\n",
    "            print(key+'\\t'+str(len(vals)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Feature selection and labeling parts of speech\n",
    "In order to optimize our classifier, we need to select 'features' of sentences for our classifier to learn from.  These, in Python, are in the form of dictionaries a la { 'feature' : 'word' }  with optional tagging thrown in for training sessions, e.g. ({'PRP':'Their'},'person') or left off, {'PRP':'Their'} for testing/validation sessions.  The <i>label_set_pos</i> function below tags each word in sentence with it's proper part of speech (POS) tag and returns the word/label tag as a dictionary.  The dictionary is then labeled with it's proper category for future classification and appended to labeled_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def label_set_pos(TRAINERS):\n",
    "    labeled_words=[]\n",
    "    for trainingSet in TRAINERS:\n",
    "        for key,vals in trainingSet.items():\n",
    "            for linkid in vals:\n",
    "                #PAGECONTENT=wikipedia.page(linkid).content\n",
    "                PAGECONTENT=wikipedia.summary(linkid)\n",
    "                PAGECONTENT=nltk.word_tokenize(PAGECONTENT)\n",
    "                POSTAG=nltk.pos_tag(PAGECONTENT)\n",
    "                labeled_words+=[(dict((b,a) for (a,b) in POSTAG),key)]\n",
    "    return labeled_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###Classifying unknowns\n",
    "This function is fairly similar to the above.  However, it does not label the Part Of Speech dictionary after being returned, since it does not what category the article belongs to, and instead returns an unlabeled dictionary of the words in the unclassified article summary.  These words are similarly tagged, using Python's Natural Language Toolkit, NLTK, and once tagged, then run through the NLTK's classify function, which returns a string with the classifiers best guess as to which category the article belongs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def classify_unknowns(classifier,TESTDATA):\n",
    "    featureset=[]\n",
    "    for article in TESTDATA:\n",
    "        #CONTENT=wikipedia.page(article).content\n",
    "        CONTENT=wikipedia.summary(article)\n",
    "        CONTENT=nltk.word_tokenize(CONTENT)\n",
    "        CONTENT=nltk.pos_tag(CONTENT)\n",
    "        featureset=dict((b,a) for (a,b) in CONTENT)\n",
    "        fs=json.dumps({article:featureset})\n",
    "        print(\"Name: {0:10}  \".format(article),\n",
    "                  \"My Guess: {0:10}\".format(classifier.classify(featureset)))\n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Start of actual work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load old classifier if it exists\n",
    "import pickle\n",
    "ifp = open('myclassifier.pickle','wb')\n",
    "classifier=pickle.load(ifp)\n",
    "ifp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person\t37\n",
      "city\t36\n",
      "movie\t27\n",
      "animal\t44\n",
      "study\t29\n",
      "holiday\t18\n"
     ]
    }
   ],
   "source": [
    "#Shows the length of the training data set we train off\n",
    "trainer_length(TRAINERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.classify.naivebayes.NaiveBayesClassifier object at 0x7f7697de3828>\n",
      "new classifier saved!\n"
     ]
    }
   ],
   "source": [
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "labeled_words=[]\n",
    "labeled_words=label_set_pos(TRAINERS)\n",
    "classifier=NaiveBayesClassifier.train(labeled_words)\n",
    "print(classifier)\n",
    "import pickle\n",
    "ofp = open('myclassifier.pickle','wb')\n",
    "pickle.dump(classifier,ofp)\n",
    "ofp.close()\n",
    "print('new classifier saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the classifier will allow us to load it later and save us the tedious process of waiting for the classfier and trainer to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     PRP = 'It'            movie : person =     13.2 : 1.0\n",
      "                    PRP$ = 'his'          person : city   =     12.0 : 1.0\n",
      "                      CD = None            study : city   =     10.6 : 1.0\n",
      "                     JJS = None            study : city   =     10.1 : 1.0\n",
      "                     VBD = 'was'            city : study  =      9.4 : 1.0\n",
      "                     VBP = None           person : animal =      8.7 : 1.0\n",
      "                     VBP = 'are'          animal : person =      8.4 : 1.0\n",
      "                     POS = None            study : city   =      8.3 : 1.0\n",
      "                      CC = 'or'           holida : city   =      8.1 : 1.0\n",
      "                     WDT = 'that'          study : city   =      7.8 : 1.0\n",
      "                      MD = 'can'           study : city   =      7.6 : 1.0\n",
      "                     VBD = None           animal : city   =      7.6 : 1.0\n",
      "                    PRP$ = 'its'            city : person =      7.6 : 1.0\n",
      "                    PRP$ = 'their'        animal : person =      7.4 : 1.0\n",
      "                    NNPS = 'States'         city : study  =      7.2 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "best_features=classifier.most_informative_features(25)\n",
    "print(classifier.show_most_informative_features(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Abraham_Lincoln   My Guess: person    \n",
      "Name: Danny_Devito   My Guess: person    \n",
      "Name: Rheumatology   My Guess: study     \n",
      "Name: Oarfish      My Guess: animal    \n",
      "Name: Lobster      My Guess: study     \n",
      "Name: United_Passions   My Guess: movie     \n",
      "Name: Air_Heads    My Guess: movie     \n",
      "Name: Benjamin_Franklin   My Guess: city      \n",
      "Name: Grace_Kelly   My Guess: person    \n",
      "Name: Blue_Whale   My Guess: animal    \n",
      "Name: Grey_Seal    My Guess: animal    \n",
      "Name: Glenn_Howerton   My Guess: person    \n",
      "Name: Charlie_Day   My Guess: person    \n",
      "Name: The_Last_Airbender   My Guess: movie     \n",
      "Name: Susan_Sarandon   My Guess: person    \n",
      "Name: Joanna_Kerns   My Guess: person    \n",
      "Name: Snowy_Owl    My Guess: animal    \n",
      "Name: Great_White_Shark   My Guess: animal    \n",
      "Name: Mako_Shark   My Guess: animal    \n",
      "Name: Mahi_Mahi    My Guess: animal    \n"
     ]
    }
   ],
   "source": [
    "classify_unknowns(classifier,TESTDATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####In this set of 20 tests, the classifier guesses 18, or just over 90%, correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
